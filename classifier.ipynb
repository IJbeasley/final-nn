{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required functions from nn module\n",
    "from nn.io import read_text_file, read_fasta_file\n",
    "from nn.nn import NeuralNetwork\n",
    "from nn.preprocess import one_hot_encode_seqs,  sample_seqs\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# for splitting data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rap1 positives: 137\n",
      "Number of Rap1 negatives: 3163\n",
      "\n",
      " First positive example: ACATCCGTGCACCTCCG\n",
      "First negative example: CTTCATGTCAGCCTGCACTTCTGGGTCGTTGAAGTTTCTACCGATCAAACGCTTAGCGTCGAAAACGGTATTCGAAGGATTCATAGCAGCTTGATTCTTAGCAGCATCACCAATCAATCTTTCAGTGTCAGTGAAAGCGACAAAAGATGGAGTGGTTCTGTTACCTTGATCGTTGGCAATAATGTCCACACGATCATTAGCAAAGTGAGCAACACACGAGTATGTTGTACCTAAATCAATACCGACAGCTTTTGACATATTATCTGTTATTTACTTGAATTTTTGTTTCTTGTAATACTTGATTACTTTTCTTTTGATGTGCTTATCTTACAAATAGAGAAAATAAAACAACTTAAGTAAGAATTGGGAAACGAAACTACAACTCAATCCCTTCTCGAAGATACATCAATCCACCCCTTATATAACCTTGAAGTCCTCGAAACGATCAGCTAATCTAAATGGCCCCCCTTCTTTTTGGGTTCTTTCTCTCCCTTTTGCCGCCGATGGAACGTTCTGGAAAAAGAAGAATAATTTAATTACTTTCTCAACTAAAATCTGGAGAAAAAACGCAAATGACAGCTTCTAAACGTTCCGTGTGCTTTCTTTCTAGAATGTTCTGGAAAGTTTACAACAATCCACAAGAACGAAAATGCCGTTGACAATGATGAAACCATCATCCACACACCGCGCACACGTGCTTTATTTCTTTTTCTGAATTTTTTTTTTCCGCCATTTTCAACCAAGGAAATTTTTTTTCTTAGGGCTCAGAACCTGCAGGTGAAGAAGCGCTTTAGAAATCAAAGCACAACGTAACAATTTGTCGACAACCGAGCCTTTGAAGAAAAAATTTTTCACATTGTCGCCTCTAAATAAATAGTTTAAGGTTATCTACCCACTATATTTAGTTGGTTCTTTTTTTTTTCCTTCTACTCTTTATCTTTTTACCTCATGCTTTCTACCTTTCAGCACTGAAGAGTCCAACCGAATATATACACACATA\n",
      "\n",
      " Average positive sequence length: 17.0\n",
      "Average negative sequence length: 999.1454315523238\n",
      "\n",
      " Min positive sequence length: 17\n",
      "Max positive sequence length: 17\n",
      "Min negative sequence length: 52\n",
      "Max negative sequence length: 1000\n"
     ]
    }
   ],
   "source": [
    "# read in the 137 positive Rap1 motif examples.\n",
    "rap1_pos = read_text_file(\"data/rap1-lieb-positives.txt\")\n",
    "\n",
    "# read in all the negative Rap1 motif  examples\n",
    "rap1_neg = read_fasta_file(\"data/yeast-upstream-1k-negative.fa\")\n",
    "\n",
    "print(\"Number of Rap1 positives: \" + str(len(rap1_pos)))\n",
    "print(\"Number of Rap1 negatives: \" + str(len(rap1_neg)))\n",
    "\n",
    "print(\"\\n First positive example: \" + rap1_pos[0])\n",
    "print(\"First negative example: \" + rap1_neg[0])\n",
    "\n",
    "# calculate the average length of the sequences\n",
    "pos_lengths = [len(seq) for seq in rap1_pos]\n",
    "neg_lengths = [len(seq) for seq in rap1_neg]\n",
    "print(\"\\n Average positive sequence length: \" + str(np.mean(pos_lengths)))\n",
    "print(\"Average negative sequence length: \" + str(np.mean(neg_lengths)))\n",
    "\n",
    "# calculate min and max lengths of the sequences\n",
    "print(\"\\n Min positive sequence length: \" + str(np.min(pos_lengths)))\n",
    "print(\"Max positive sequence length: \" + str(np.max(pos_lengths)))\n",
    "print(\"Min negative sequence length: \" + str(np.min(neg_lengths)))\n",
    "print(\"Max negative sequence length: \" + str(np.max(neg_lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2: Balance your classes using your sample_seq function and explain why you chose the sampling scheme you did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New min length of negative sequences: 17\n",
      "New max length of negative sequences: 17\n",
      "\n",
      " Number of positive examples are sampling: 1370\n",
      "Number of negative examples are sampling: 1370\n"
     ]
    }
   ],
   "source": [
    "# from min and max calculations in above cell all rap1 positive sequences are 17 bp long, shorter than every negative sequence\n",
    "# so we will need to trim the negative sequences to 17 bp \n",
    "\n",
    "# Trim the negative sequences to 17 bp - by randomly selecting a 17 bp region from each sequence\n",
    "# Choose random region - rather than just the first or last 17 bp - to avoid biasing the different regions of the sequence\n",
    "# e.g. if the starts of these sequences are biased in some way, then always selecting the first 17 bp would bias the negative sequences\n",
    "# in a way that is not present in the positive sequences\n",
    "np.random.seed(42)\n",
    "#rap1_neg = [seq[np.random.randint(0, len(seq) - 17):np.random.randint(17, len(seq))] for seq in rap1_neg]\n",
    "rap1_neg = [seq[start : start + 17] for seq in rap1_neg if len(seq) >= 17 \n",
    "                    for start in [np.random.randint(0, len(seq) - 16)]]\n",
    "\n",
    "# Check that the lengths are now all 17 bp\n",
    "neg_lengths = [len(seq) for seq in rap1_neg]\n",
    "print(\"New min length of negative sequences: \" + str(np.min(neg_lengths)))\n",
    "print(\"New max length of negative sequences: \" + str(np.max(neg_lengths)))\n",
    "\n",
    "# Now we have sequences of the same length, let's balance the classes\n",
    "\n",
    "# the class imbalance here is huge, so we will first downsample the negative class a little, as otherwise we would have to sample the positive class many times\n",
    "# to balance the classes (creating long training, and weird biases) - let's make the imbalance 1:10\n",
    "sampled_neg_indices = np.random.choice(len(rap1_neg), len(rap1_pos) * 10)\n",
    "rap1_neg = [rap1_neg[i] for i in sampled_neg_indices]\n",
    "\n",
    "\n",
    "# Now let's combine positive and negative classes \n",
    "# so that we can balance the classes by sampling with replacement from the positive class\n",
    "rap1 = rap1_pos  + rap1_neg\n",
    "labels = [1] * len(rap1_pos) + [0] * len(rap1_neg)\n",
    "\n",
    "# Up sample the positive class by sampling with replacement\n",
    "rap1, labels = sample_seqs(rap1, labels)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "rap1 = np.array(rap1)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Generate a shuffled index\n",
    "shuffle_indices = np.random.permutation(len(rap1))\n",
    "\n",
    "# Apply the shuffled indices\n",
    "rap1_shuffled = rap1[shuffle_indices]\n",
    "labels_shuffled = labels[shuffle_indices]\n",
    "\n",
    "# Check class balance\n",
    "print(\"\\n Number of positive examples are sampling: \" + str(np.sum(labels_shuffled)))\n",
    "print(\"Number of negative examples are sampling: \" + str(len(labels_shuffled) - np.sum(labels_shuffled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3: One-hot encode the data using your one_hot_encode_seqs function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoding changed number of sequences feature matrix shape from: 2740 to 2740\n",
      "For first sequence, one-hot-encoding changed the length from: 17 to 68\n"
     ]
    }
   ],
   "source": [
    "X = one_hot_encode_seqs(rap1_shuffled)\n",
    "y = labels_shuffled\n",
    "print(\"One hot encoding changed number of sequences feature matrix shape from: \" + str(len(rap1_shuffled)) + \" to \" + str(len(X )))\n",
    "print(\"For first sequence, one-hot-encoding changed the length from: \" + str(len(rap1_shuffled[0])) + \" to \" + str(len(X[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 4: Split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, labels_shuffled, test_size = 0.1, random_state =42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 5: Generate an instance of your NeuralNetwork class with an appropriate architecture.\n",
    "## step 6: Train your neural network on the training data.\n",
    "\n",
    "Generating an instance of NeuralNetwork class requires choosing the below parameters: \n",
    "- nn_arch (neural net architecture), including dimensions and activation function per layer\n",
    "- lr (learning rate)\n",
    "- batch_size (size of mini-batches used for training)\n",
    "- loss_function (mean squared error or binary cross entropy)\n",
    "- epochs (max number for training)\n",
    "\n",
    "### How did I select hyperparameter values?\n",
    "\n",
    "For **epochs**: I initially choose a large number (500) as training for this dataset doesn't take a prohibitatively long time for this each . I then doubled check that this number was sufficent by looking at the plot of validation and training loss per epoch. since the loss appears to converge (not change much over the last few epochs), then the number of epochs chosen was likely appropriate. \n",
    "\n",
    "For **loss_function**: I chose bce instead of mse as the data we trying to predict / replicate is binary (i.e. a classification  not regression problem) where this loss function is more appropriate.\n",
    "\n",
    "For **batch_size**: I knew I needed to choose a small enough batch size (relative to the size of the training dataset) so I could perform training within the memory constraints of my laptop. However, the batch size also needed to be large enough to prevent stochasticity from inhibiting training.  I thus took inspiration from the default auto setting in sklearn's MLPRegressor function (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor), and initially set batch size as 200. I then measured the time it took for the model to train on my laptop, and the checked the below plot loss per epoch of training and validation. since the time it took for the model to train was not prohibitative, and the loss per epoch didn't 'bounce' around a lot, I thought this batch size was acceptable. However, in the future, performing a grid search to select a value would be ideal.   \n",
    "\n",
    "For **learning_rate**: I took inspiration from the default setting in sklearn's MLPRegressor function (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor), and initially set the learning rate to be 0.001. similarly, in the future, performing a grid search with at least a few other learning rates (0.01, 0.001, 0.0001) to select a value would be ideal.  \n",
    "\n",
    "For **nn_arch**: \n",
    "\n",
    "since we are trying to predict postive or negative Rap1, I chose 'sigmoid' as the activation function for the output layer as it makes it obvious to decided where to make the threshold of positive and negative values (since it restricts values between zero and one). Then, for all all hidden layers, I set the activation function to be 'relu'. I chose this as 'relu' introduces sparsity which can aid neural networks in classification tasks. \n",
    "\n",
    "For the purposes of this assignment I chose a simple neural archecture of 3 layers. The first layer was of 17 neurons, corresponding to the number of nucleotide bases (before one-hot-encoding), the second layer was of 4 neurons (the smallest tf motif length), and the last layer was of one neuron (as we are doing binary classification). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be a 64x16x64 autoencoder architecture\n",
    "nn_arch = [{\"input_dim\": 68, \"output_dim\": 17 , \"activation\": \"relu\"},\n",
    "                      {\"input_dim\": 17, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "                      {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"relu\"}]\n",
    "\n",
    "#batch size\n",
    "batch_size = 200\n",
    "\n",
    "# learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# number of epochs\n",
    "epochs = 500\n",
    "\n",
    "# loss function\n",
    "loss_function = \"bce\"\n",
    "\n",
    "# Initialize the neural network structure\n",
    "nn_model = NeuralNetwork(nn_arch = nn_arch, lr = lr, epochs = epochs, batch_size = batch_size, loss_function=loss_function,\n",
    "                                              seed = 42\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 7: Plot your training and validation loss by epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below code adapted from wk 7 regression logreg.py  \n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(8, 8))\n",
    "fig.suptitle('Training Loss History')\n",
    "axs[0].plot(np.arange(len(per_epoch_loss_train)), per_epoch_loss_train)\n",
    "axs[0].set_title('Training')\n",
    "axs[1].plot(np.arange(len(per_epoch_loss_vall)), per_epoch_loss_val)\n",
    "axs[1].set_title('Validation')\n",
    "plt.xlabel('Epoch')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 8: Report the accuracy of your classifier on your validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(per_epoch_loss_val[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 9: Explain your choice of loss function and hyperparameters.\n",
    "\n",
    "see text under step 5, generate an instance of your NeuralNetwork class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GMM-Tute",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
