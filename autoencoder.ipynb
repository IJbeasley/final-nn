{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages to import\n",
    "import numpy as np\n",
    "\n",
    "# getting the digits dataset\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# our neural network class / module\n",
    "from nn.nn import NeuralNetwork\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1. Load the digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structure of digits dataset\n",
      "(1797, 64)\n",
      "(1797,)\n",
      "\n",
      " first sample\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "\n",
      " Plot first sample\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "print(\"structure of digits dataset\")\n",
    "print(digits.data.shape) # 1797 samples, 64 features  - X matrix\n",
    "print(digits.target.shape) # 1797 samples - y (labels) vector\n",
    "\n",
    "print(\"\\n first sample\")\n",
    "print(digits.data[0])  # first sample \n",
    "\n",
    "# plot the first sample\n",
    "print(\"\\n Plot first sample\")\n",
    "plt.imshow(digits.data[0].reshape(8, 8), cmap='gray')\n",
    "plt.title(f\"Label: {digits.target[0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2:  splt into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (instances, features)  (1617, 64)\n",
      "Validation set: (instances, features)  (180, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(digits[\"data\"],\n",
    "                                                                       digits[\"target\"],\n",
    "                                                                       test_size = 0.1, \n",
    "                                                                       random_state = 42\n",
    "                                                                       )\n",
    "\n",
    "print(\"Training set: (instances, features) \", X_train.shape)\n",
    "print(\"Validation set: (instances, features) \", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3: Generate an instance of your NeuralNetwork class with a 64x16x64 autoencoder architecture.\n",
    "\n",
    "Generating an instance of NeuralNetwork class requires choosing the below parameters: \n",
    "- nn_arch (neural net architecture), including dimensions and activation function per layer\n",
    "- lr (learning rate)\n",
    "- batch_size (size of mini-batches used for training)\n",
    "- loss_function (mean squared error or binary cross entropy)\n",
    "- epochs (max number for training)\n",
    "\n",
    "### How did I select hyperparameter values?\n",
    "\n",
    "For **epochs**: I initially choose a large number (500) as training for this dataset doesn't take a prohibitatively long time for this each . I then doubled check that this number was sufficent by looking at the plot of validation and training loss per epoch. since the loss appears to converge (not change much over the last few epochs), then the number of epochs chosen was likely appropriate. **TO ADD**\n",
    "\n",
    "For **loss_function**: I chose mse instead of bce as the data we trying to predict / replicate is continous (i.e. a regression not classification problem) where this loss function is more appropriate.\n",
    "\n",
    "For **batch_size**: I knew I needed to choose a small enough batch size (relative to the size of the training dataset) so I could perform training within the memory constraints of my laptop. However, the batch size also needed to be large enough to prevent stochasticity from inhibiting training.  I thus took inspiration from the default auto setting in sklearn's MLPRegressor function (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor), and initially set batch size as 200. I then measured the time it took for the model to train on my laptop, and the checked the below plot loss per epoch of training and validation. since the time it took for the model to train was not prohibitative, and the loss per epoch didn't 'bounce' around a lot, I thought this batch size was acceptable. However, in the future, performing a grid search to select a value would be ideal.   \n",
    "\n",
    "For **learning_rate**: I took inspiration from the default setting in sklearn's MLPRegressor function (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor), and initially set the learning rate to be 0.001. similarly, in the future, performing a grid search with at least a few other learning rates (0.01, 0.001, 0.0001) to select a value would be ideal.  \n",
    "\n",
    "For **nn_arch**: since the input features varied outside of the range 0 and 1, I chose 'relu' as the activation function for the output layer to help the output data better resemble the input data ('sigmoid' would have restricted the output to be between 0 and 1). For all hidden layers, I set the activation function to be 'relu'. For the purposes of this assignment I chose the simplest neural archecture that would fufill 64x16x64 encoder architecture requirement. However, in the future it would be best to perform a grid search on the training set to investigate whether adding extra hidden layers would improve model performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize the neural network structure\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m nn_model \u001b[38;5;241m=\u001b[39m \u001b[43mNeuralNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_arch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnn_arch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ijfbe\\SSD_Documents\\final-nn\\nn\\nn.py:52\u001b[0m, in \u001b[0;36mNeuralNetwork.__init__\u001b[1;34m(self, nn_arch, lr, seed, batch_size, epochs, loss_function)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Initialize the parameter dictionary for use in training\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ijfbe\\SSD_Documents\\final-nn\\nn\\nn.py:76\u001b[0m, in \u001b[0;36mNeuralNetwork._init_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39march):\n\u001b[0;32m     75\u001b[0m     layer_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 76\u001b[0m     input_dim \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     77\u001b[0m     output_dim \u001b[38;5;241m=\u001b[39m layer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_dim\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     78\u001b[0m     param_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(layer_idx)] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(output_dim, input_dim) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters\n",
    "\n",
    "# neural network architecture\n",
    "# needs to be a 64x16x64 autoencoder architecture\n",
    "nn_arch = {2: [{\"input_dim\": 64, \"output_dim\": 16, \"activation\": \"relu\"},\n",
    "                      {\"input_dim\": 16, \"output_dim\": 64, \"activation\": \"relu\"}]\n",
    "                  }\n",
    "\n",
    "#batch size\n",
    "batch_size = 200\n",
    "\n",
    "# learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# number of epochs\n",
    "epochs = 500\n",
    "\n",
    "# loss function\n",
    "loss_function = \"mse\"\n",
    "\n",
    "# Initialize the neural network structure\n",
    "nn_model = NeuralNetwork(nn_arch = nn_arch, lr = lr, epochs = epochs, batch_size = batch_size, loss_function=loss_function,\n",
    "                                              seed = 42\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 4: Train your autoencoder on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit / train the model\n",
    "per_epoch_loss_train, per_epoch_loss_val = nn_model.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 5: Plot your training and validation loss by epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below code adapted from wk 7 regression logreg.py  \n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(8, 8))\n",
    "fig.suptitle('Training Loss History')\n",
    "axs[0].plot(np.arange(len(per_epoch_loss_train)), per_epoch_loss_train)\n",
    "axs[0].set_title('Training')\n",
    "axs[1].plot(np.arange(len(per_epoch_loss_vall)), per_epoch_loss_val)\n",
    "axs[1].set_title('Validation')\n",
    "plt.xlabel('Epoch')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 6: Quantify your average reconstruction error over the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(per_epoch_loss_val[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 7: Explain why you chose the hyperparameter values you did.\n",
    "\n",
    "see text under step 3, generate an instance of your NeuralNetwork class with a 64x16x64 autoencoder architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GMM-Tute",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
